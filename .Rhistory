mutate(disciplines = calcul_discipline(reference_complete)) %>%
mutate(nb_disciplines = str_split(disciplines, " / ")[[1]] %>% length()),
ANX4$i_2_chap_ouvrages %>%
clean_names() %>%
select(reference_complete) %>%
rowwise() %>%
mutate(nb_copubli = calcul_nb_copubli(reference_complete)) %>%
mutate(disciplines = calcul_discipline(reference_complete)) %>%
mutate(nb_disciplines = str_split(disciplines, " / ")[[1]] %>% length())
) %>%
filter(nb_copubli > 1 & nb_disciplines == 1)
solo <- ANX4$i_1_articles_sctfq %>%
clean_names() %>%
select(reference_complete) %>%
rowwise() %>%
mutate(nb_copubli = calcul_nb_copubli(reference_complete)) %>%
filter(nb_copubli == 1)
SOLO
solo
HAL <- jsonlite::fromJSON("data/ETBX_2017_2020.json")$response$docs %>% tibble()
HAL %>%
filter(docType_s %in% c("ART", "COUV")) %>%
rowwise() %>%
mutate(Acronymes = list(c(c_across(contains("Acronym"))))) %>%
mutate(Noms = list(c(c_across(contains("StructName"))))) %>%
mutate(Pays = list(c(c_across(contains("Country"))))) %>%
pull(Noms) %>%
unlist() %>%
unique() -> structures
HAL %>%
count(docType_s) %>%
select(Type = docType_s, N = n)
table_collab <- readxl::read_excel("data/Structures_Collab_BH.xlsx") %>% clean_names()
table_collab
table_collab %>% count(pays)
table_collab %>% count(pays) %>% arrange(desc(n))
table_collab %>% count(pays) %>% arrange(desc(n)) %>% drop_na()
table_collab %>% count(pays) %>% arrange(desc(n)) %>% drop_na() %>% top_n(10)
table_collab
table_collab distinct(sigle)
table_collab distinct(sigle)
table_collab %>%  distinct(sigle)
# Chunk 1: setup
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, out.width = "100%", dpi = 320)
# Chunk 2: packages
library(dplyr)
library(tidyr)
library(lubridate)
library(janitor)
library(ggplot2)
library(readxl)
library(purrr)
library(bib2df)
library(wordcloud2)
library(stringr)
library(readr)
source("R/theme_inrae.R")
# Chunk 3
# Fichier en date du 03/08/2020
file <- "data/Annexe4_ETBX_complet_2020_08_03.xlsx"
# On réalise une boucle pour importer tous les onglets dans un seul objet, sous forme de liste
sheet_names <- readxl::excel_sheets(file)
ANX4 <- list()
for (i in sheet_names[-1:-3]) {
ANX4[[i]] <- readxl::read_excel(file, sheet = i, skip = 1) %>%
select(-1) # Retrait colonne n°
}
# On rend exploitables les noms d'onglets
names(ANX4) <- janitor::make_clean_names(names(ANX4))
# Chunk 4
replace_cases <- function(x) {
value <- ifelse(is.na(x), yes = 0, no = 1)
return(value)
}
# Chunk 5
tab_dim <- tibble(
Onglet = names(ANX4),
nb_lignes = map_dbl(ANX4, nrow),
nb_colonnes = map_dbl(ANX4, ncol)
) %>%
arrange(desc(nb_lignes))
# On ne va garder que les onglets qui ne sont pas vides.
# Les onglets à 2 lignes sont à chaque fois vide (car la colonne n° a été remplie pour 1 et 2)
# sauf pour 4 onglets particuliers qui sont ici rajoutés.
Onglets_non_empty <- tab_dim %>%
filter(nb_lignes != 2) %>%
pull(Onglet) %>%
c("ii_3_activ_consult", "iii_1_elearning", "i_9_contrats_internationaux", "i_1_articles_synth")
# On affiche le tableau (Seulement le top 10)
tab_dim %>%
filter(Onglet %in% Onglets_non_empty) %>%
slice(1:10)
# Chunk 6
## Extraction des projets nationaux
projets_nationaux <- ANX4$i_9_contrats_nationaux %>%
clean_names() %>%
select(-x11) %>%
drop_na(contrat) %>%
drop_na(date_debut) %>%
mutate(type = "National")
## Projets européens
projets_europ <- ANX4$i_9_contrats_europ_autres %>%
drop_na(`Date début`) %>%
clean_names() %>%
mutate_at(vars(date_debut:date_fin), as.Date) %>%
mutate(type = "Européen")
## Projets internationaux
projets_inter <- ANX4$i_9_contrats_internationaux %>%
clean_names() %>%
mutate_at(vars(date_debut:date_fin), as.Date) %>%
mutate(type = "International")
## Projets R&D
projets_rd <- ANX4$i_9_contrats_prive_r_d_indus %>%
clean_names() %>%
mutate_at(vars(date_debut:date_fin), as.Date) %>%
mutate(type = "R&D")
## Projets PIA
projets_pia <- ANX4$i_9_contrats_pia %>%
clean_names() %>%
drop_na(contrat, date_debut) %>%
mutate_at(vars(date_debut:date_fin), as.Date) %>%
mutate(type = "PIA")
## Projets de collectivités territoriales
projets_coll_terri <- ANX4$i_9_contrats_coll_territ %>%
clean_names() %>%
drop_na(contrat, date_debut, date_fin) %>%
mutate_at(vars(date_debut), as.Date, origin = "1899-12-31") %>%
mutate(type = "National")
## On assemble le tout
PRJ <- bind_rows(projets_nationaux, projets_europ) %>%
bind_rows(projets_inter) %>%
bind_rows(projets_rd) %>%
bind_rows(projets_pia) %>%
bind_rows(projets_coll_terri) %>%
mutate_at(vars(porteur:axe_3), replace_cases) %>%
unique() %>%
mutate(date_fin = replace_na(date_fin, "2024-01-01")) %>%
mutate(porteur = recode(porteur, "0" = "Non porteur", "1" = "Porteur")) %>%
mutate(porteur = factor(porteur, levels = c("Porteur", "Non porteur"))) %>%
group_by(contrat) %>%
summarise(
date_debut = min(date_debut),
date_fin = max(date_fin),
porteur = unique(porteur),
type = unique(type)
) %>%
ungroup() %>%
arrange(desc(date_debut)) %>%
mutate(contrat = factor(contrat, levels = unique(contrat)))
# Chunk 7
ggplot(PRJ, aes(x = date_fin, y = contrat)) +
geom_segment(aes(x = date_debut, xend = date_fin, y = contrat, yend = contrat, color = type, linetype = porteur), size = 1.5) +
scale_y_discrete(limits = rev(levels(PRJ$contrat))) +
geom_point(fill = "black", color = "black", size = 3) +
geom_point(aes(x = date_debut, y = contrat), color = "black", fill = "black", size = 3) +
theme_inrae() +
theme(axis.text.y = element_text(size = 10)) +
geom_vline(xintercept = as.Date("2020-06-01"), color = "blue", size = 4) +
labs(x = "Temps", y = "Contrats", color = "Type de contrat", linetype = "ETBX porteur ?")
# Chunk 8
PRJ %>% count(porteur)
PRJ %>% count(type)
# Chunk 9
PRJ %>%
mutate(
annee_debut = lubridate::year(date_debut),
annee_fin = lubridate::year(date_fin)
) %>%
group_by(annee_debut) %>%
count(type) %>%
spread(key = annee_debut, value = n) %>%
ungroup() %>%
mutate_at(vars(`2013`:`2019`), replace_na, 0)
# Chunk 10
PRJ %>%
mutate(
annee_debut = lubridate::year(date_debut),
annee_fin = lubridate::year(date_fin)
) %>%
group_by(annee_fin) %>%
count(type) %>%
spread(key = annee_fin, value = n) %>%
ungroup() %>%
mutate_at(vars(`2017`:`2025`), replace_na, 0)
# Chunk 11
par_an <- PRJ %>%
mutate(
annee_debut = lubridate::year(date_debut),
annee_fin = lubridate::year(date_fin)
)
l <- list()
for (i in 2014:2020) {
l[[as.character(i)]] <- par_an %>%
filter(annee_debut <= i & annee_fin >= i) %>%
mutate(an = i)
}
count_type <- function(df) {
df %>%
group_by(type, an) %>%
count() %>%
arrange(desc(n))
}
map(l, count_type) %>%
bind_rows() %>%
spread(key = an, value = n) %>%
ungroup() %>%
mutate_at(vars(`2014`:`2020`), replace_na, 0)
# Chunk 12
acl1 <- ANX4$i_1_articles_sctfq %>%
clean_names() %>%
mutate(year = stringr::str_extract(reference_complete, "\\d{4}")) %>%
drop_na(year) %>%
group_by(year) %>%
summarise(n = n_distinct(reference_complete)) %>%
spread(key = year, value = n) %>%
mutate(Type = "Articles")
acl2 <- ANX4$i_1_autres_articles %>%
clean_names() %>%
mutate(year = stringr::str_extract(reference_complete, "\\d{4}")) %>%
filter(year != 2016) %>%
drop_na(year) %>%
group_by(year) %>%
summarise(n = n_distinct(reference_complete)) %>%
spread(key = year, value = n) %>%
mutate(Type = "Autres articles")
acl3 <- ANX4$i_3_articles_actes_colloq %>%
clean_names() %>%
mutate(year = stringr::str_extract(reference_complete, "\\d{4}")) %>%
drop_na(year) %>%
group_by(year) %>%
summarise(n = n_distinct(reference_complete)) %>%
spread(key = year, value = n) %>%
mutate(Type = "Actes colloques")
acl4 <- ANX4$i_2_chap_ouvrages %>%
clean_names() %>%
mutate(year = stringr::str_extract(reference_complete, "\\d{4}")) %>%
drop_na(year) %>%
group_by(year) %>%
summarise(n = n_distinct(reference_complete)) %>%
spread(key = year, value = n) %>%
mutate(Type = "Chapitres ouvrages")
bind_rows(acl1, acl2, acl3, acl4) %>%
select(Type, `2017`:`2020`)
# Chunk 13
nb_art_ang <- ANX4$i_1_articles_sctfq %>%
clean_names() %>%
filter(articles_scientifiques_en_anglais_ou_dans_une_autre_langue_etrangere_shs_uniquement == "x") %>%
nrow()
# ANX4$i_1_autres_articles %>%
#   clean_names() %>%
#   filter(autres_articles_articles_publies_dans_des_revues_professionnelles_ou_techniques_etc_en_anglais_ou_dans_une_autre_langue_etrangere_shs_uniquement == "x") %>%
#   nrow()
nb_ouv_ang <- ANX4$i_2_chap_ouvrages %>%
clean_names() %>%
filter(chapitres_d_ouvrage_en_anglais_ou_dans_une_autre_langue_etrangere == "x") %>%
nrow()
# Chunk 14
clean_revues <- function(x) {
r <- case_when(
x == "water research, elsevier" ~ "water research",
x == "water resources research, agu" ~ "water resources research",
x == "water science and technology: water supply, iwa" ~ "water science and technology: water supply",
x == "vertigo - la revue électronique en sciences de l'environnement 1" ~ "vertigo",
x == "revue internationale des etudes du développement" ~ "revue internationale des etudes du developpement",
x == "journal of hydroinformatics, iwa" ~ "journal of hydroinformatics",
x == TRUE ~ x
)
if (is.na(r)) {
return(x)
}
return(r)
}
word_count <- ANX4$i_1_articles_sctfq %>%
clean_names() %>%
mutate(journal = clean_revues(journal)) %>%
group_by(journal) %>%
count() %>%
arrange(desc(n)) %>%
ungroup() %>%
rowwise() %>%
mutate(n = ifelse(journal == "Journal of Water Resources Planning and Management", yes = 7, no = n)) %>%
mutate(journal = ifelse(journal == "Journal of Water Resources Planning and Management", yes = "Water Res. Planning and Management", no = journal)) %>%
ungroup() %>%
mutate(journal = str_to_lower(journal)) %>%
mutate(journal = str_trim(journal))
# wordcloud2(word_count, size = 0.35)
# Chunk 15
articles <- ANX4$i_1_articles_sctfq %>%
clean_names() %>%
mutate(journal = clean_revues(journal)) %>%
group_by(journal) %>%
count() %>%
arrange(desc(n)) %>%
mutate(journal = str_to_lower(journal)) %>%
mutate(journal = str_trim(journal))
tab_relecture_articles <- ANX4$i_8_evaluation_articles %>%
clean_names() %>%
select(revue_ouvrage, nombre_de_relectures) %>%
mutate(revue_ouvrage = str_to_lower(revue_ouvrage)) %>%
mutate(revue_ouvrage = str_trim(revue_ouvrage)) %>%
full_join(articles, by = c("revue_ouvrage" = "journal")) %>%
arrange(revue_ouvrage) %>%
unique() %>%
mutate(revue_ouvrage = clean_revues(revue_ouvrage)) %>%
group_by(revue_ouvrage) %>%
summarise(n_relecture = sum(nombre_de_relectures, na.rm = TRUE), n_publi = sum(n, na.rm = TRUE)) %>%
ungroup() %>%
unique()
# Chunk 16
tab_relecture_articles %>%
mutate(diff = abs(n_publi - n_relecture)) %>%
filter(diff > 1) %>%
ggplot(aes(x = reorder(revue_ouvrage, -n_publi))) +
geom_segment(aes(
x = reorder(revue_ouvrage, -n_publi), xend = reorder(revue_ouvrage, -n_publi),
y = n_publi, yend = n_relecture
)) +
geom_point(aes(y = n_publi, fill = "Nombre de publications"), color = "black", shape = 21, size = 4, alpha = 0.8) +
geom_point(aes(y = n_relecture, fill = "Nombre de relectures"), color = "black", shape = 21, size = 4, alpha = 0.8) +
theme_inrae() +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
labs(x = "Revues", y = "Nombre", fill = "Type") +
scale_y_continuous(breaks = seq(0, 30, 2)) +
labs(caption = "NB : par souci de lisibilité, seules les revues pour lesquelles la différence entre les deux variables est supérieure à 1 sont affichées")
# Chunk 17
tab_relecture_articles %>%
mutate(diff = abs(n_publi - n_relecture)) %>%
filter(diff > 1) %>%
ggplot(aes(x = reorder(revue_ouvrage, -n_publi))) +
geom_hline(yintercept = 0, size = 2) +
geom_segment(aes(
x = reorder(revue_ouvrage, -n_publi), xend = reorder(revue_ouvrage, -n_publi),
y = n_publi, yend = -n_relecture
)) +
geom_point(aes(y = n_publi, fill = "Nombre de publications"), color = "black", shape = 21, size = 4, alpha = 0.8) +
geom_point(aes(y = -n_relecture, fill = "Nombre de relectures"), color = "black", shape = 21, size = 4, alpha = 0.8) +
theme_inrae() +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
labs(x = "Revues", y = "Nombre", fill = "Type") +
scale_y_continuous(breaks = seq(-30, 12, 2)) +
labs(caption = "NB : par souci de lisibilité, seules les revues pour lesquelles la différence entre les deux variables est supérieure à 1 sont affichées")
# Chunk 18
HAL <- jsonlite::fromJSON("data/ETBX_2017_2020.json")$response$docs %>% tibble()
# Chunk 19
base_doi <- HAL %>%
drop_na(doiId_s) %>%
distinct(doiId_s)
## On crée la requête
# base_doi %>%
#   mutate(request = paste0("DOI(",doiId_s,")")) %>%
#   pull(request) %>%
#   paste0(collapse = " OR ")
# Chunk 20
read_csv("data/scopus.csv") %>%
clean_names() %>%
group_by(year) %>%
summarise(n_citations = sum(cited_by, na.rm = TRUE)) %>%
select(`Année` = year, n_citations)
# Chunk 21
table_auteurs <- readxl::read_excel("data/Table_auteurs_ETBX_2020-08-17_BH.xlsx") %>% clean_names()
liste_auteurs_etbx <- table_auteurs %>%
filter(etbx_oui_non %in% c("oui", "temporaire", "oui / BSA")) %>%
pull(auteur)
liste_auteurs_etbx
# Chunk 22
calcul_nb_copubli <- function(x) {
liste_auteurs_etbx[str_detect(x, liste_auteurs_etbx)] %>%
gsub("^\\.|\\.$", "", .) %>%
unique() %>%
length()
}
tab_copubli <- bind_rows(
ANX4$i_1_articles_sctfq %>%
clean_names() %>%
select(reference_complete) %>%
rowwise() %>%
mutate(nb_copubli = calcul_nb_copubli(reference_complete)) %>%
ungroup() %>%
arrange(desc(nb_copubli)),
ANX4$i_2_chap_ouvrages %>%
clean_names() %>%
select(reference_complete) %>%
rowwise() %>%
mutate(nb_copubli = calcul_nb_copubli(reference_complete)) %>%
ungroup() %>%
arrange(desc(nb_copubli))
) %>%
filter(nb_copubli > 1)
ggplot(tab_copubli, aes(x = nb_copubli)) +
geom_histogram(fill = "#00a3a6", color = "black", binwidth = 1) +
theme_inrae() +
scale_y_continuous(breaks = seq(0, 40, 2)) +
labs(x = "Nombre de co-auteurs ETBX sur une production scientifique", y = "Fréquence")
# Chunk 23
word_count <- ANX4$i_1_articles_sctfq %>%
clean_names() %>%
select(reference_complete, journal) %>%
rowwise() %>%
mutate(nb_copubli = calcul_nb_copubli(reference_complete)) %>%
ungroup() %>%
mutate(journal = clean_revues(journal)) %>%
group_by(journal) %>%
summarise(nb_moyen = mean(nb_copubli)) %>%
ungroup() %>%
arrange(desc(nb_moyen))
# wordcloud2(word_count, size = 0.35)
# Chunk 24: categ_jcr
liste_revues_jcr <- list()
for (i in list.files("data/revues")) {
nom <- str_remove_all(i, ".csv")
liste_revues_jcr[[nom]] <- read_csv(file.path("data/revues/", i), skip = 1) %>%
tibble() %>%
janitor::clean_names() %>%
select(full_journal_title, total_cites, journal_impact_factor) %>%
mutate(
total_cites = as.numeric(total_cites),
journal_impact_factor = as.numeric(journal_impact_factor)
) %>%
mutate(CATEGORY = nom)
}
table_jcr <- bind_rows(liste_revues_jcr) %>%
mutate(full_journal_title = str_to_lower(full_journal_title)) %>%
unique()
tab_relecture_articles$revue_ouvrage[tab_relecture_articles$revue_ouvrage %in% table_jcr$full_journal_title]
# Chunk 25
tab_relecture_articles %>%
inner_join(table_jcr, by = c("revue_ouvrage" = "full_journal_title")) %>%
group_by(CATEGORY) %>%
count() %>%
ungroup() %>%
ggplot(aes(x = reorder(CATEGORY, n), y = n)) +
geom_col(fill = "#00a3a6", color = "black") +
geom_label(aes(label = n)) +
coord_flip() +
theme_inrae() +
labs(y = "Nombre de publications", x = "Catégorie JCR")
# Chunk 26
table_disciplines_auteurs <- table_auteurs %>%
filter(etbx_oui_non %in% c("oui", "temporaire", "oui / BSA")) %>%
select(auteur, discipline) %>%
drop_na()
calcul_discipline <- function(x) {
df <- data.frame(auteur = liste_auteurs_etbx[str_detect(x, liste_auteurs_etbx)] %>% gsub("^\\.|\\.$", "", .) %>% unique())
df %>%
inner_join(table_disciplines_auteurs, by = "auteur") %>%
pull(discipline) %>%
unique() %>%
paste(collapse = " / ")
}
tab_disciplines <- bind_rows(
ANX4$i_1_articles_sctfq %>%
clean_names() %>%
select(reference_complete) %>%
rowwise() %>%
mutate(disciplines = calcul_discipline(reference_complete)) %>%
mutate(nb_disciplines = str_split(disciplines, " / ")[[1]] %>% length()) %>%
ungroup(),
ANX4$i_2_chap_ouvrages %>%
clean_names() %>%
select(reference_complete) %>%
rowwise() %>%
mutate(disciplines = calcul_discipline(reference_complete)) %>%
mutate(nb_disciplines = str_split(disciplines, " / ")[[1]] %>% length()) %>%
ungroup()
) %>% filter(nb_disciplines > 1)
ggplot(tab_disciplines, aes(x = nb_disciplines)) +
geom_histogram(fill = "#00a3a6", color = "black", binwidth = 1) +
theme_inrae() +
scale_y_continuous(breaks = seq(0, 40, 2)) +
labs(x = "Nombre de disciplines impliquées sur une production scientifique", y = "Fréquence")
# Chunk 27
mono_dis <- bind_rows(
ANX4$i_1_articles_sctfq %>%
clean_names() %>%
select(reference_complete) %>%
rowwise() %>%
mutate(nb_copubli = calcul_nb_copubli(reference_complete)) %>%
mutate(disciplines = calcul_discipline(reference_complete)) %>%
mutate(nb_disciplines = str_split(disciplines, " / ")[[1]] %>% length()),
ANX4$i_2_chap_ouvrages %>%
clean_names() %>%
select(reference_complete) %>%
rowwise() %>%
mutate(nb_copubli = calcul_nb_copubli(reference_complete)) %>%
mutate(disciplines = calcul_discipline(reference_complete)) %>%
mutate(nb_disciplines = str_split(disciplines, " / ")[[1]] %>% length())
) %>%
filter(nb_copubli > 1 & nb_disciplines == 1)
# Chunk 28
solo <- ANX4$i_1_articles_sctfq %>%
clean_names() %>%
select(reference_complete) %>%
rowwise() %>%
mutate(nb_copubli = calcul_nb_copubli(reference_complete)) %>%
filter(nb_copubli == 1)
# Chunk 29
HAL <- jsonlite::fromJSON("data/ETBX_2017_2020.json")$response$docs %>% tibble()
HAL %>%
filter(docType_s %in% c("ART", "COUV")) %>%
rowwise() %>%
mutate(Acronymes = list(c(c_across(contains("Acronym"))))) %>%
mutate(Noms = list(c(c_across(contains("StructName"))))) %>%
mutate(Pays = list(c(c_across(contains("Country"))))) %>%
pull(Noms) %>%
unlist() %>%
unique() -> structures
HAL %>%
count(docType_s) %>%
select(Type = docType_s, N = n)
# Chunk 30
table_collab <- readxl::read_excel("data/Structures_Collab_BH.xlsx") %>% clean_names()
table_collab
